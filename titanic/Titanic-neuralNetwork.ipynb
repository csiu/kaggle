{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAY 02 - Feb 26, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network\n",
    "**Goal:** To implement a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = \"./data/\"\n",
    "train_file = os.path.join(input_dir, \"train.csv\")\n",
    "test_file = os.path.join(input_dir, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "That data I'll be using is the [Titanic dataset from the Kaggle competition](https://www.kaggle.com/c/titanic). The goal of this competition is to predict whether or not a passenger survived the sinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A glimpse of the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows & columns\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    891\n",
       "Survived       891\n",
       "Pclass         891\n",
       "Name           891\n",
       "Sex            891\n",
       "Age            714\n",
       "SibSp          891\n",
       "Parch          891\n",
       "Ticket         891\n",
       "Fare           891\n",
       "Cabin          204\n",
       "Embarked       889\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId        NaN\n",
       "Survived             0\n",
       "Pclass               3\n",
       "Name               NaN\n",
       "Sex               male\n",
       "Age                 24\n",
       "SibSp                0\n",
       "Parch                0\n",
       "Ticket            1601\n",
       "Fare              8.05\n",
       "Cabin          B96 B98\n",
       "Embarked             S\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mode (for filling na)\n",
    "modes = train.mode().iloc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace NA\n",
    "train.Age.fillna(modes[\"Age\"], inplace=True)\n",
    "train.Embarked.fillna(modes[\"Embarked\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map categorical variable to integer\n",
    "train.Sex.replace({\"male\":0, \"female\":1}, inplace=True)\n",
    "train.Embarked.replace({\"C\":0, \"Q\":1, \"S\":2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
       "0         0       3    0  22.0      1      0   7.2500\n",
       "1         1       1    1  38.0      1      0  71.2833\n",
       "2         1       3    1  26.0      0      0   7.9250\n",
       "3         1       1    1  35.0      1      0  53.1000\n",
       "4         0       3    0  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove categorical variables & passenger ID to obtain features\n",
    "train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    891\n",
       "Pclass      891\n",
       "Sex         891\n",
       "Age         891\n",
       "SibSp       891\n",
       "Parch       891\n",
       "Fare        891\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that there are no NAs\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple 2-layer neural network\n",
    "- Code reference: KAI XUAN WEI's [A simple 2-layer neural network model\n",
    "](https://www.kaggle.com/vandermode/digit-recognizer/a-simple-2-layer-neural-network-model)\n",
    "- Andrew Trusk's [A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/) might also be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.    ,   3.    ,   0.    , ...,   1.    ,   0.    ,   7.25  ],\n",
       "       [  1.    ,   1.    ,   1.    , ...,   1.    ,   0.    ,  71.2833],\n",
       "       [  1.    ,   3.    ,   1.    , ...,   0.    ,   0.    ,   7.925 ],\n",
       "       ..., \n",
       "       [  0.    ,   3.    ,   1.    , ...,   1.    ,   2.    ,  23.45  ],\n",
       "       [  1.    ,   1.    ,   0.    , ...,   0.    ,   0.    ,  30.    ],\n",
       "       [  0.    ,   3.    ,   0.    , ...,   0.    ,   0.    ,   7.75  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data from dataframe\n",
    "data = train.as_matrix()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training set and validation set\n",
    "y = data[:, 0].astype(int)\n",
    "X = data[:, 1:].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  6\n",
      "train:  801\n",
      "valid:  90\n"
     ]
    }
   ],
   "source": [
    "train_num = int(X.shape[0]*.9)\n",
    "val_num = X.shape[0] - train_num\n",
    "\n",
    "print(\"features: \", X.shape[1])\n",
    "print(\"train: \", train_num)\n",
    "print(\"valid: \", val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = X[:train_num], y[:train_num]\n",
    "X_val, y_val = X[train_num:], y[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "mean_pixel = X_train.mean(axis=0)\n",
    "X_train -= mean_pixel\n",
    "X_val -= mean_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For the following line containing `N, D = train_num, 6`, the 6 refers to the number of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An simple 2-layers full-connected neural network model\n",
    "# Note we only use numpy \n",
    "\n",
    "# Initialize our nn\n",
    "def initialize_global_weights():\n",
    "    global W1, b1, W2, b2\n",
    "    N, D = train_num, 6\n",
    "    H, C = 500, 2\n",
    "    W1 = 0.001 * np.random.rand(D, H)\n",
    "    b1 = np.zeros(H)\n",
    "    W2 = 0.001 * np.random.rand(H, C)\n",
    "    b2 = np.zeros(C)\n",
    "\n",
    "initialize_global_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement our train function\n",
    "def train_or_evaluate(X, y=None, loss_fn=None, lr=1e-3, reg=0.0):\n",
    "    global W1, W2, b1, b2\n",
    "    # forward pass\n",
    "    a = X.dot(W1) + b1\n",
    "    scores = a.dot(W2) + b2\n",
    "    if y is None:\n",
    "        return scores\n",
    "    loss, dscores = loss_fn(scores, y)\n",
    "    print('loss: %f' % loss)\n",
    "    # backward pass\n",
    "    dW2 = np.dot(a.T, dscores) + reg * W2\n",
    "    db2 = np.sum(dscores, axis=0)\n",
    "    da = np.dot(dscores, W2.T)\n",
    "    db1 = np.sum(da, axis=0)\n",
    "    dW1 = np.dot(X.T, da) + reg * W1\n",
    "    # update params\n",
    "    W1 += - lr * dW1\n",
    "    W2 += - lr * dW2\n",
    "    b1 += - lr * db1\n",
    "    b2 += - lr * db2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement our softmax loss function\n",
    "def softmax(scores, y):\n",
    "    N = scores.shape[0]\n",
    "    scores = scores.copy()\n",
    "    scores -= np.max(scores, axis=1)[:, None]\n",
    "    probs = np.exp(scores)\n",
    "    probs /= np.sum(probs, axis=1)[:, None]\n",
    "    loss = np.sum(-np.log(probs[np.arange(N), y])) / N\n",
    "    \n",
    "    dscores = probs.copy()\n",
    "    dscores[np.arange(N), y] -= 1\n",
    "    \n",
    "    return loss, dscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646691635456\n"
     ]
    }
   ],
   "source": [
    "# Use initialized weight to checkout train accuracy\n",
    "scores = train_or_evaluate(X_train)\n",
    "print((np.argmax(scores, axis=1) == y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.693130\n",
      "loss: 0.693127\n",
      "loss: 0.693123\n",
      "loss: 0.693120\n",
      "loss: 0.693116\n",
      "loss: 0.693113\n",
      "loss: 0.693110\n",
      "loss: 0.693106\n",
      "loss: 0.693103\n",
      "loss: 0.693100\n",
      "loss: 0.693096\n",
      "loss: 0.693093\n",
      "loss: 0.693089\n",
      "loss: 0.693086\n",
      "loss: 0.693083\n",
      "loss: 0.693079\n",
      "loss: 0.693076\n",
      "loss: 0.693073\n",
      "loss: 0.693069\n",
      "loss: 0.693066\n",
      "loss: 0.693062\n",
      "loss: 0.693059\n",
      "loss: 0.693056\n",
      "loss: 0.693052\n",
      "loss: 0.693049\n",
      "loss: 0.693046\n",
      "loss: 0.693042\n",
      "loss: 0.693039\n",
      "loss: 0.693035\n",
      "loss: 0.693032\n",
      "loss: 0.693029\n",
      "loss: 0.693025\n",
      "loss: 0.693022\n",
      "loss: 0.693019\n",
      "loss: 0.693015\n",
      "loss: 0.693012\n",
      "loss: 0.693008\n",
      "loss: 0.693005\n",
      "loss: 0.693002\n",
      "loss: 0.692998\n",
      "loss: 0.692995\n",
      "loss: 0.692992\n",
      "loss: 0.692988\n",
      "loss: 0.692985\n",
      "loss: 0.692982\n",
      "loss: 0.692978\n",
      "loss: 0.692975\n",
      "loss: 0.692971\n",
      "loss: 0.692968\n",
      "loss: 0.692965\n",
      "loss: 0.692961\n",
      "loss: 0.692958\n",
      "loss: 0.692955\n",
      "loss: 0.692951\n",
      "loss: 0.692948\n",
      "loss: 0.692944\n",
      "loss: 0.692941\n",
      "loss: 0.692938\n",
      "loss: 0.692934\n",
      "loss: 0.692931\n",
      "loss: 0.692928\n",
      "loss: 0.692924\n",
      "loss: 0.692921\n",
      "loss: 0.692918\n",
      "loss: 0.692914\n",
      "loss: 0.692911\n",
      "loss: 0.692907\n",
      "loss: 0.692904\n",
      "loss: 0.692901\n",
      "loss: 0.692897\n",
      "loss: 0.692894\n",
      "loss: 0.692891\n",
      "loss: 0.692887\n",
      "loss: 0.692884\n",
      "loss: 0.692880\n",
      "loss: 0.692877\n",
      "loss: 0.692874\n",
      "loss: 0.692870\n",
      "loss: 0.692867\n",
      "loss: 0.692864\n",
      "loss: 0.692860\n",
      "loss: 0.692857\n",
      "loss: 0.692854\n",
      "loss: 0.692850\n",
      "loss: 0.692847\n",
      "loss: 0.692843\n",
      "loss: 0.692840\n",
      "loss: 0.692837\n",
      "loss: 0.692833\n",
      "loss: 0.692830\n",
      "loss: 0.692827\n",
      "loss: 0.692823\n",
      "loss: 0.692820\n",
      "loss: 0.692816\n",
      "loss: 0.692813\n",
      "loss: 0.692810\n",
      "loss: 0.692806\n",
      "loss: 0.692803\n",
      "loss: 0.692800\n",
      "loss: 0.692796\n",
      "loss: 0.692793\n",
      "loss: 0.692790\n",
      "loss: 0.692786\n",
      "loss: 0.692783\n",
      "loss: 0.692779\n",
      "loss: 0.692776\n",
      "loss: 0.692773\n",
      "loss: 0.692769\n",
      "loss: 0.692766\n",
      "loss: 0.692763\n",
      "loss: 0.692759\n",
      "loss: 0.692756\n",
      "loss: 0.692752\n",
      "loss: 0.692749\n",
      "loss: 0.692746\n",
      "loss: 0.692742\n",
      "loss: 0.692739\n",
      "loss: 0.692736\n",
      "loss: 0.692732\n",
      "loss: 0.692729\n",
      "loss: 0.692726\n",
      "loss: 0.692722\n",
      "loss: 0.692719\n",
      "loss: 0.692715\n",
      "loss: 0.692712\n",
      "loss: 0.692709\n",
      "loss: 0.692705\n",
      "loss: 0.692702\n",
      "loss: 0.692699\n",
      "loss: 0.692695\n",
      "loss: 0.692692\n",
      "loss: 0.692688\n",
      "loss: 0.692685\n",
      "loss: 0.692682\n",
      "loss: 0.692678\n",
      "loss: 0.692675\n",
      "loss: 0.692672\n",
      "loss: 0.692668\n",
      "loss: 0.692665\n",
      "loss: 0.692661\n",
      "loss: 0.692658\n",
      "loss: 0.692655\n",
      "loss: 0.692651\n",
      "loss: 0.692648\n",
      "loss: 0.692645\n",
      "loss: 0.692641\n",
      "loss: 0.692638\n",
      "loss: 0.692634\n",
      "loss: 0.692631\n",
      "loss: 0.692628\n",
      "loss: 0.692624\n",
      "loss: 0.692621\n",
      "loss: 0.692618\n",
      "loss: 0.692614\n",
      "loss: 0.692611\n",
      "loss: 0.692607\n",
      "loss: 0.692604\n",
      "loss: 0.692601\n",
      "loss: 0.692597\n",
      "loss: 0.692594\n",
      "loss: 0.692591\n",
      "loss: 0.692587\n",
      "loss: 0.692584\n",
      "loss: 0.692580\n",
      "loss: 0.692577\n",
      "loss: 0.692574\n",
      "loss: 0.692570\n",
      "loss: 0.692567\n",
      "loss: 0.692564\n",
      "loss: 0.692560\n",
      "loss: 0.692557\n",
      "loss: 0.692553\n",
      "loss: 0.692550\n",
      "loss: 0.692547\n",
      "loss: 0.692543\n",
      "loss: 0.692540\n",
      "loss: 0.692537\n",
      "loss: 0.692533\n",
      "loss: 0.692530\n",
      "loss: 0.692526\n",
      "loss: 0.692523\n",
      "loss: 0.692520\n",
      "loss: 0.692516\n",
      "loss: 0.692513\n",
      "loss: 0.692509\n",
      "loss: 0.692506\n",
      "loss: 0.692503\n",
      "loss: 0.692499\n",
      "loss: 0.692496\n",
      "loss: 0.692493\n",
      "loss: 0.692489\n",
      "loss: 0.692486\n",
      "loss: 0.692482\n",
      "loss: 0.692479\n",
      "loss: 0.692476\n",
      "loss: 0.692472\n",
      "loss: 0.692469\n",
      "loss: 0.692465\n",
      "loss: 0.692462\n",
      "loss: 0.692459\n",
      "loss: 0.692455\n",
      "loss: 0.692452\n",
      "loss: 0.692449\n",
      "loss: 0.692445\n",
      "loss: 0.692442\n",
      "loss: 0.692438\n",
      "loss: 0.692435\n",
      "loss: 0.692432\n",
      "loss: 0.692428\n",
      "loss: 0.692425\n",
      "loss: 0.692421\n",
      "loss: 0.692418\n",
      "loss: 0.692415\n",
      "loss: 0.692411\n",
      "loss: 0.692408\n",
      "loss: 0.692405\n",
      "loss: 0.692401\n",
      "loss: 0.692398\n",
      "loss: 0.692394\n",
      "loss: 0.692391\n",
      "loss: 0.692388\n",
      "loss: 0.692384\n",
      "loss: 0.692381\n",
      "loss: 0.692377\n",
      "loss: 0.692374\n",
      "loss: 0.692371\n",
      "loss: 0.692367\n",
      "loss: 0.692364\n",
      "loss: 0.692360\n",
      "loss: 0.692357\n",
      "loss: 0.692354\n",
      "loss: 0.692350\n",
      "loss: 0.692347\n",
      "loss: 0.692343\n",
      "loss: 0.692340\n",
      "loss: 0.692337\n",
      "loss: 0.692333\n",
      "loss: 0.692330\n",
      "loss: 0.692326\n",
      "loss: 0.692323\n",
      "loss: 0.692320\n",
      "loss: 0.692316\n",
      "loss: 0.692313\n",
      "loss: 0.692309\n",
      "loss: 0.692306\n",
      "loss: 0.692303\n",
      "loss: 0.692299\n",
      "loss: 0.692296\n",
      "loss: 0.692292\n",
      "loss: 0.692289\n",
      "loss: 0.692286\n",
      "loss: 0.692282\n",
      "loss: 0.692279\n",
      "loss: 0.692275\n",
      "loss: 0.692272\n",
      "loss: 0.692269\n",
      "loss: 0.692265\n",
      "loss: 0.692262\n",
      "loss: 0.692258\n",
      "loss: 0.692255\n",
      "loss: 0.692251\n",
      "loss: 0.692248\n",
      "loss: 0.692245\n",
      "loss: 0.692241\n",
      "loss: 0.692238\n",
      "loss: 0.692234\n",
      "loss: 0.692231\n",
      "loss: 0.692228\n",
      "loss: 0.692224\n",
      "loss: 0.692221\n",
      "loss: 0.692217\n",
      "loss: 0.692214\n",
      "loss: 0.692211\n",
      "loss: 0.692207\n",
      "loss: 0.692204\n",
      "loss: 0.692200\n",
      "loss: 0.692197\n",
      "loss: 0.692193\n",
      "loss: 0.692190\n",
      "loss: 0.692187\n",
      "loss: 0.692183\n",
      "loss: 0.692180\n",
      "loss: 0.692176\n",
      "loss: 0.692173\n",
      "loss: 0.692170\n",
      "loss: 0.692166\n",
      "loss: 0.692163\n",
      "loss: 0.692159\n",
      "loss: 0.692156\n",
      "loss: 0.692152\n",
      "loss: 0.692149\n",
      "loss: 0.692146\n",
      "loss: 0.692142\n",
      "loss: 0.692139\n",
      "loss: 0.692135\n",
      "loss: 0.692132\n",
      "loss: 0.692128\n",
      "loss: 0.692125\n",
      "loss: 0.692122\n",
      "loss: 0.692118\n",
      "loss: 0.692115\n",
      "loss: 0.692111\n",
      "loss: 0.692108\n",
      "loss: 0.692104\n",
      "loss: 0.692101\n",
      "loss: 0.692098\n",
      "loss: 0.692094\n",
      "loss: 0.692091\n",
      "loss: 0.692087\n",
      "loss: 0.692084\n",
      "loss: 0.692080\n",
      "loss: 0.692077\n",
      "loss: 0.692074\n",
      "loss: 0.692070\n",
      "loss: 0.692067\n",
      "loss: 0.692063\n",
      "loss: 0.692060\n",
      "loss: 0.692056\n",
      "loss: 0.692053\n",
      "loss: 0.692049\n",
      "loss: 0.692046\n",
      "loss: 0.692043\n",
      "loss: 0.692039\n",
      "loss: 0.692036\n",
      "loss: 0.692032\n",
      "loss: 0.692029\n",
      "loss: 0.692025\n",
      "loss: 0.692022\n",
      "loss: 0.692018\n",
      "loss: 0.692015\n",
      "loss: 0.692012\n",
      "loss: 0.692008\n",
      "loss: 0.692005\n",
      "loss: 0.692001\n",
      "loss: 0.691998\n",
      "loss: 0.691994\n",
      "loss: 0.691991\n",
      "loss: 0.691987\n",
      "loss: 0.691984\n",
      "loss: 0.691980\n",
      "loss: 0.691977\n",
      "loss: 0.691974\n",
      "loss: 0.691970\n",
      "loss: 0.691967\n",
      "loss: 0.691963\n",
      "loss: 0.691960\n",
      "loss: 0.691956\n",
      "loss: 0.691953\n",
      "loss: 0.691949\n",
      "loss: 0.691946\n",
      "loss: 0.691942\n",
      "loss: 0.691939\n",
      "loss: 0.691936\n",
      "loss: 0.691932\n",
      "loss: 0.691929\n",
      "loss: 0.691925\n",
      "loss: 0.691922\n",
      "loss: 0.691918\n",
      "loss: 0.691915\n",
      "loss: 0.691911\n",
      "loss: 0.691908\n",
      "loss: 0.691904\n",
      "loss: 0.691901\n",
      "loss: 0.691897\n",
      "loss: 0.691894\n",
      "loss: 0.691890\n",
      "loss: 0.691887\n",
      "loss: 0.691884\n",
      "loss: 0.691880\n",
      "loss: 0.691877\n",
      "loss: 0.691873\n",
      "loss: 0.691870\n",
      "loss: 0.691866\n",
      "loss: 0.691863\n",
      "loss: 0.691859\n",
      "loss: 0.691856\n",
      "loss: 0.691852\n",
      "loss: 0.691849\n",
      "loss: 0.691845\n",
      "loss: 0.691842\n",
      "loss: 0.691838\n",
      "loss: 0.691835\n",
      "loss: 0.691831\n",
      "loss: 0.691828\n",
      "loss: 0.691824\n",
      "loss: 0.691821\n",
      "loss: 0.691817\n",
      "loss: 0.691814\n",
      "loss: 0.691810\n",
      "loss: 0.691807\n",
      "loss: 0.691803\n",
      "loss: 0.691800\n",
      "loss: 0.691796\n",
      "loss: 0.691793\n",
      "loss: 0.691789\n",
      "loss: 0.691786\n",
      "loss: 0.691782\n",
      "loss: 0.691779\n",
      "loss: 0.691775\n",
      "loss: 0.691772\n",
      "loss: 0.691768\n",
      "loss: 0.691765\n",
      "loss: 0.691761\n",
      "loss: 0.691758\n",
      "loss: 0.691754\n",
      "loss: 0.691751\n",
      "loss: 0.691747\n",
      "loss: 0.691744\n",
      "loss: 0.691740\n",
      "loss: 0.691737\n",
      "loss: 0.691733\n",
      "loss: 0.691730\n",
      "loss: 0.691726\n",
      "loss: 0.691723\n",
      "loss: 0.691719\n",
      "loss: 0.691716\n",
      "loss: 0.691712\n",
      "loss: 0.691709\n",
      "loss: 0.691705\n",
      "loss: 0.691702\n",
      "loss: 0.691698\n",
      "loss: 0.691695\n",
      "loss: 0.691691\n",
      "loss: 0.691688\n",
      "loss: 0.691684\n",
      "loss: 0.691681\n",
      "loss: 0.691677\n",
      "loss: 0.691674\n",
      "loss: 0.691670\n",
      "loss: 0.691667\n",
      "loss: 0.691663\n",
      "loss: 0.691660\n",
      "loss: 0.691656\n",
      "loss: 0.691653\n",
      "loss: 0.691649\n",
      "loss: 0.691645\n",
      "loss: 0.691642\n",
      "loss: 0.691638\n",
      "loss: 0.691635\n",
      "loss: 0.691631\n",
      "loss: 0.691628\n",
      "loss: 0.691624\n",
      "loss: 0.691621\n",
      "loss: 0.691617\n",
      "loss: 0.691614\n",
      "loss: 0.691610\n",
      "loss: 0.691607\n",
      "loss: 0.691603\n",
      "loss: 0.691600\n",
      "loss: 0.691596\n",
      "loss: 0.691592\n",
      "loss: 0.691589\n",
      "loss: 0.691585\n",
      "loss: 0.691582\n",
      "loss: 0.691578\n",
      "loss: 0.691575\n",
      "loss: 0.691571\n",
      "loss: 0.691568\n",
      "loss: 0.691564\n",
      "loss: 0.691560\n",
      "loss: 0.691557\n",
      "loss: 0.691553\n",
      "loss: 0.691550\n",
      "loss: 0.691546\n",
      "loss: 0.691543\n",
      "loss: 0.691539\n",
      "loss: 0.691536\n",
      "loss: 0.691532\n",
      "loss: 0.691528\n",
      "loss: 0.691525\n",
      "loss: 0.691521\n",
      "loss: 0.691518\n",
      "loss: 0.691514\n",
      "loss: 0.691511\n",
      "loss: 0.691507\n",
      "loss: 0.691504\n",
      "loss: 0.691500\n",
      "loss: 0.691496\n",
      "loss: 0.691493\n",
      "loss: 0.691489\n",
      "loss: 0.691486\n",
      "loss: 0.691482\n",
      "loss: 0.691479\n",
      "loss: 0.691475\n",
      "loss: 0.691471\n",
      "loss: 0.691468\n",
      "loss: 0.691464\n",
      "loss: 0.691461\n",
      "loss: 0.691457\n",
      "loss: 0.691453\n",
      "loss: 0.691450\n",
      "loss: 0.691446\n",
      "loss: 0.691443\n",
      "loss: 0.691439\n",
      "loss: 0.691436\n",
      "loss: 0.691432\n",
      "loss: 0.691428\n",
      "loss: 0.691425\n",
      "loss: 0.691421\n",
      "loss: 0.691418\n",
      "loss: 0.691414\n",
      "loss: 0.691410\n",
      "loss: 0.691407\n",
      "loss: 0.691403\n",
      "loss: 0.691400\n",
      "loss: 0.691396\n",
      "loss: 0.691392\n",
      "loss: 0.691389\n",
      "loss: 0.691385\n",
      "loss: 0.691382\n",
      "loss: 0.691378\n",
      "loss: 0.691374\n",
      "loss: 0.691371\n",
      "loss: 0.691367\n",
      "loss: 0.691364\n",
      "loss: 0.691360\n",
      "loss: 0.691356\n",
      "loss: 0.691353\n",
      "loss: 0.691349\n",
      "loss: 0.691345\n",
      "loss: 0.691342\n",
      "loss: 0.691338\n",
      "loss: 0.691335\n",
      "loss: 0.691331\n",
      "loss: 0.691327\n",
      "loss: 0.691324\n",
      "loss: 0.691320\n",
      "loss: 0.691317\n",
      "loss: 0.691313\n",
      "loss: 0.691309\n",
      "loss: 0.691306\n",
      "loss: 0.691302\n",
      "loss: 0.691298\n",
      "loss: 0.691295\n",
      "loss: 0.691291\n",
      "loss: 0.691287\n",
      "loss: 0.691284\n",
      "loss: 0.691280\n",
      "loss: 0.691277\n",
      "loss: 0.691273\n",
      "loss: 0.691269\n",
      "loss: 0.691266\n",
      "loss: 0.691262\n",
      "loss: 0.691258\n",
      "loss: 0.691255\n",
      "loss: 0.691251\n",
      "loss: 0.691247\n",
      "loss: 0.691244\n",
      "loss: 0.691240\n",
      "loss: 0.691236\n",
      "loss: 0.691233\n",
      "loss: 0.691229\n",
      "loss: 0.691225\n",
      "loss: 0.691222\n",
      "loss: 0.691218\n",
      "loss: 0.691215\n",
      "loss: 0.691211\n",
      "loss: 0.691207\n",
      "loss: 0.691204\n",
      "loss: 0.691200\n",
      "loss: 0.691196\n",
      "loss: 0.691193\n",
      "loss: 0.691189\n",
      "loss: 0.691185\n",
      "loss: 0.691182\n",
      "loss: 0.691178\n",
      "loss: 0.691174\n",
      "loss: 0.691171\n",
      "loss: 0.691167\n",
      "loss: 0.691163\n",
      "loss: 0.691159\n",
      "loss: 0.691156\n",
      "loss: 0.691152\n",
      "loss: 0.691148\n",
      "loss: 0.691145\n",
      "loss: 0.691141\n",
      "loss: 0.691137\n",
      "loss: 0.691134\n",
      "loss: 0.691130\n",
      "loss: 0.691126\n",
      "loss: 0.691123\n",
      "loss: 0.691119\n",
      "loss: 0.691115\n",
      "loss: 0.691112\n",
      "loss: 0.691108\n",
      "loss: 0.691104\n",
      "loss: 0.691100\n",
      "loss: 0.691097\n",
      "loss: 0.691093\n",
      "loss: 0.691089\n",
      "loss: 0.691086\n",
      "loss: 0.691082\n",
      "loss: 0.691078\n",
      "loss: 0.691075\n",
      "loss: 0.691071\n",
      "loss: 0.691067\n",
      "loss: 0.691063\n",
      "loss: 0.691060\n",
      "loss: 0.691056\n",
      "loss: 0.691052\n",
      "loss: 0.691049\n",
      "loss: 0.691045\n",
      "loss: 0.691041\n",
      "loss: 0.691037\n",
      "loss: 0.691034\n",
      "loss: 0.691030\n",
      "loss: 0.691026\n",
      "loss: 0.691022\n",
      "loss: 0.691019\n",
      "loss: 0.691015\n",
      "loss: 0.691011\n",
      "loss: 0.691008\n",
      "loss: 0.691004\n",
      "loss: 0.691000\n",
      "loss: 0.690996\n",
      "loss: 0.690993\n",
      "loss: 0.690989\n",
      "loss: 0.690985\n",
      "loss: 0.690981\n",
      "loss: 0.690978\n",
      "loss: 0.690974\n",
      "loss: 0.690970\n",
      "loss: 0.690966\n",
      "loss: 0.690963\n",
      "loss: 0.690959\n",
      "loss: 0.690955\n",
      "loss: 0.690951\n",
      "loss: 0.690948\n",
      "loss: 0.690944\n",
      "loss: 0.690940\n",
      "loss: 0.690936\n",
      "loss: 0.690933\n",
      "loss: 0.690929\n",
      "loss: 0.690925\n",
      "loss: 0.690921\n",
      "loss: 0.690917\n",
      "loss: 0.690914\n",
      "loss: 0.690910\n",
      "loss: 0.690906\n",
      "loss: 0.690902\n",
      "loss: 0.690899\n",
      "loss: 0.690895\n",
      "loss: 0.690891\n",
      "loss: 0.690887\n",
      "loss: 0.690883\n",
      "loss: 0.690880\n",
      "loss: 0.690876\n",
      "loss: 0.690872\n",
      "loss: 0.690868\n",
      "loss: 0.690865\n",
      "loss: 0.690861\n",
      "loss: 0.690857\n",
      "loss: 0.690853\n",
      "loss: 0.690849\n",
      "loss: 0.690846\n",
      "loss: 0.690842\n",
      "loss: 0.690838\n",
      "loss: 0.690834\n",
      "loss: 0.690830\n",
      "loss: 0.690827\n",
      "loss: 0.690823\n",
      "loss: 0.690819\n",
      "loss: 0.690815\n",
      "loss: 0.690811\n",
      "loss: 0.690807\n",
      "loss: 0.690804\n",
      "loss: 0.690800\n",
      "loss: 0.690796\n",
      "loss: 0.690792\n",
      "loss: 0.690788\n",
      "loss: 0.690785\n",
      "loss: 0.690781\n",
      "loss: 0.690777\n",
      "loss: 0.690773\n",
      "loss: 0.690769\n",
      "loss: 0.690765\n",
      "loss: 0.690762\n",
      "loss: 0.690758\n",
      "loss: 0.690754\n",
      "loss: 0.690750\n",
      "loss: 0.690746\n",
      "loss: 0.690742\n",
      "loss: 0.690739\n",
      "loss: 0.690735\n",
      "loss: 0.690731\n",
      "loss: 0.690727\n",
      "loss: 0.690723\n",
      "loss: 0.690719\n",
      "loss: 0.690715\n",
      "loss: 0.690712\n",
      "loss: 0.690708\n",
      "loss: 0.690704\n",
      "loss: 0.690700\n",
      "loss: 0.690696\n",
      "loss: 0.690692\n",
      "loss: 0.690688\n",
      "loss: 0.690685\n",
      "loss: 0.690681\n",
      "loss: 0.690677\n",
      "loss: 0.690673\n",
      "loss: 0.690669\n",
      "loss: 0.690665\n",
      "loss: 0.690661\n",
      "loss: 0.690657\n",
      "loss: 0.690654\n",
      "loss: 0.690650\n",
      "loss: 0.690646\n",
      "loss: 0.690642\n",
      "loss: 0.690638\n",
      "loss: 0.690634\n",
      "loss: 0.690630\n",
      "loss: 0.690626\n",
      "loss: 0.690623\n",
      "loss: 0.690619\n",
      "loss: 0.690615\n",
      "loss: 0.690611\n",
      "loss: 0.690607\n",
      "loss: 0.690603\n",
      "loss: 0.690599\n",
      "loss: 0.690595\n",
      "loss: 0.690591\n",
      "loss: 0.690587\n",
      "loss: 0.690583\n",
      "loss: 0.690580\n",
      "loss: 0.690576\n",
      "loss: 0.690572\n",
      "loss: 0.690568\n",
      "loss: 0.690564\n",
      "loss: 0.690560\n",
      "loss: 0.690556\n",
      "loss: 0.690552\n",
      "loss: 0.690548\n",
      "loss: 0.690544\n",
      "loss: 0.690540\n",
      "loss: 0.690536\n",
      "loss: 0.690532\n",
      "loss: 0.690529\n",
      "loss: 0.690525\n",
      "loss: 0.690521\n",
      "loss: 0.690517\n",
      "loss: 0.690513\n",
      "loss: 0.690509\n",
      "loss: 0.690505\n",
      "loss: 0.690501\n",
      "loss: 0.690497\n",
      "loss: 0.690493\n",
      "loss: 0.690489\n",
      "loss: 0.690485\n",
      "loss: 0.690481\n",
      "loss: 0.690477\n",
      "loss: 0.690473\n",
      "loss: 0.690469\n",
      "loss: 0.690465\n",
      "loss: 0.690461\n",
      "loss: 0.690457\n",
      "loss: 0.690453\n",
      "loss: 0.690449\n",
      "loss: 0.690445\n",
      "loss: 0.690442\n",
      "loss: 0.690438\n",
      "loss: 0.690434\n",
      "loss: 0.690430\n",
      "loss: 0.690426\n",
      "loss: 0.690422\n",
      "loss: 0.690418\n",
      "loss: 0.690414\n",
      "loss: 0.690410\n",
      "loss: 0.690406\n",
      "loss: 0.690402\n",
      "loss: 0.690398\n",
      "loss: 0.690394\n",
      "loss: 0.690390\n",
      "loss: 0.690386\n",
      "loss: 0.690382\n",
      "loss: 0.690378\n",
      "loss: 0.690374\n",
      "loss: 0.690370\n",
      "loss: 0.690366\n",
      "loss: 0.690362\n",
      "loss: 0.690358\n",
      "loss: 0.690354\n",
      "loss: 0.690350\n",
      "loss: 0.690346\n",
      "loss: 0.690341\n",
      "loss: 0.690337\n",
      "loss: 0.690333\n",
      "loss: 0.690329\n",
      "loss: 0.690325\n",
      "loss: 0.690321\n",
      "loss: 0.690317\n",
      "loss: 0.690313\n",
      "loss: 0.690309\n",
      "loss: 0.690305\n",
      "loss: 0.690301\n",
      "loss: 0.690297\n",
      "loss: 0.690293\n",
      "loss: 0.690289\n",
      "loss: 0.690285\n",
      "loss: 0.690281\n",
      "loss: 0.690277\n",
      "loss: 0.690273\n",
      "loss: 0.690269\n",
      "loss: 0.690265\n",
      "loss: 0.690261\n",
      "loss: 0.690257\n",
      "loss: 0.690252\n",
      "loss: 0.690248\n",
      "loss: 0.690244\n",
      "loss: 0.690240\n",
      "loss: 0.690236\n",
      "loss: 0.690232\n",
      "loss: 0.690228\n",
      "loss: 0.690224\n",
      "loss: 0.690220\n",
      "loss: 0.690216\n",
      "loss: 0.690212\n",
      "loss: 0.690208\n",
      "loss: 0.690203\n",
      "loss: 0.690199\n",
      "loss: 0.690195\n",
      "loss: 0.690191\n",
      "loss: 0.690187\n",
      "loss: 0.690183\n",
      "loss: 0.690179\n",
      "loss: 0.690175\n",
      "loss: 0.690171\n",
      "loss: 0.690167\n",
      "loss: 0.690162\n",
      "loss: 0.690158\n",
      "loss: 0.690154\n",
      "loss: 0.690150\n",
      "loss: 0.690146\n",
      "loss: 0.690142\n",
      "loss: 0.690138\n",
      "loss: 0.690134\n",
      "loss: 0.690129\n",
      "loss: 0.690125\n",
      "loss: 0.690121\n",
      "loss: 0.690117\n",
      "loss: 0.690113\n",
      "loss: 0.690109\n",
      "loss: 0.690105\n",
      "loss: 0.690100\n",
      "loss: 0.690096\n",
      "loss: 0.690092\n",
      "loss: 0.690088\n",
      "loss: 0.690084\n",
      "loss: 0.690080\n",
      "loss: 0.690076\n",
      "loss: 0.690071\n",
      "loss: 0.690067\n",
      "loss: 0.690063\n",
      "loss: 0.690059\n",
      "loss: 0.690055\n",
      "loss: 0.690051\n",
      "loss: 0.690046\n",
      "loss: 0.690042\n",
      "loss: 0.690038\n",
      "loss: 0.690034\n",
      "loss: 0.690030\n",
      "loss: 0.690025\n",
      "loss: 0.690021\n",
      "loss: 0.690017\n",
      "loss: 0.690013\n",
      "loss: 0.690009\n",
      "loss: 0.690004\n",
      "loss: 0.690000\n",
      "loss: 0.689996\n",
      "loss: 0.689992\n",
      "loss: 0.689988\n",
      "loss: 0.689983\n",
      "loss: 0.689979\n",
      "loss: 0.689975\n",
      "loss: 0.689971\n",
      "loss: 0.689967\n",
      "loss: 0.689962\n",
      "loss: 0.689958\n",
      "loss: 0.689954\n",
      "loss: 0.689950\n",
      "loss: 0.689945\n",
      "loss: 0.689941\n",
      "loss: 0.689937\n",
      "loss: 0.689933\n",
      "loss: 0.689929\n",
      "loss: 0.689924\n",
      "loss: 0.689920\n",
      "loss: 0.689916\n",
      "loss: 0.689912\n",
      "loss: 0.689907\n",
      "loss: 0.689903\n",
      "loss: 0.689899\n",
      "loss: 0.689895\n",
      "loss: 0.689890\n",
      "loss: 0.689886\n",
      "loss: 0.689882\n",
      "loss: 0.689877\n",
      "loss: 0.689873\n",
      "loss: 0.689869\n",
      "loss: 0.689865\n",
      "loss: 0.689860\n",
      "loss: 0.689856\n",
      "loss: 0.689852\n",
      "loss: 0.689848\n",
      "loss: 0.689843\n",
      "loss: 0.689839\n",
      "loss: 0.689835\n",
      "loss: 0.689830\n",
      "loss: 0.689826\n",
      "loss: 0.689822\n",
      "loss: 0.689818\n",
      "loss: 0.689813\n",
      "loss: 0.689809\n",
      "loss: 0.689805\n",
      "loss: 0.689800\n",
      "loss: 0.689796\n",
      "loss: 0.689792\n",
      "loss: 0.689787\n",
      "loss: 0.689783\n",
      "loss: 0.689779\n",
      "loss: 0.689774\n",
      "loss: 0.689770\n",
      "loss: 0.689766\n",
      "loss: 0.689761\n",
      "loss: 0.689757\n",
      "loss: 0.689753\n",
      "loss: 0.689748\n",
      "loss: 0.689744\n",
      "loss: 0.689740\n",
      "loss: 0.689735\n",
      "loss: 0.689731\n",
      "loss: 0.689727\n",
      "loss: 0.689722\n",
      "loss: 0.689718\n",
      "loss: 0.689714\n",
      "loss: 0.689709\n",
      "loss: 0.689705\n",
      "loss: 0.689700\n",
      "loss: 0.689696\n",
      "loss: 0.689692\n",
      "loss: 0.689687\n",
      "loss: 0.689683\n",
      "loss: 0.689679\n",
      "loss: 0.689674\n",
      "loss: 0.689670\n",
      "loss: 0.689665\n",
      "loss: 0.689661\n",
      "loss: 0.689657\n",
      "loss: 0.689652\n",
      "loss: 0.689648\n",
      "loss: 0.689644\n",
      "loss: 0.689639\n",
      "loss: 0.689635\n",
      "loss: 0.689630\n",
      "loss: 0.689626\n",
      "loss: 0.689621\n",
      "loss: 0.689617\n",
      "loss: 0.689613\n",
      "loss: 0.689608\n",
      "loss: 0.689604\n",
      "loss: 0.689599\n",
      "loss: 0.689595\n",
      "loss: 0.689591\n",
      "loss: 0.689586\n",
      "loss: 0.689582\n",
      "loss: 0.689577\n",
      "loss: 0.689573\n",
      "loss: 0.689568\n",
      "loss: 0.689564\n",
      "loss: 0.689559\n",
      "loss: 0.689555\n",
      "loss: 0.689551\n",
      "loss: 0.689546\n",
      "loss: 0.689542\n",
      "loss: 0.689537\n",
      "loss: 0.689533\n",
      "loss: 0.689528\n",
      "loss: 0.689524\n",
      "loss: 0.689519\n",
      "loss: 0.689515\n",
      "loss: 0.689510\n",
      "loss: 0.689506\n",
      "loss: 0.689501\n",
      "loss: 0.689497\n",
      "loss: 0.689492\n",
      "loss: 0.689488\n",
      "loss: 0.689483\n",
      "loss: 0.689479\n",
      "loss: 0.689474\n",
      "loss: 0.689470\n",
      "loss: 0.689465\n",
      "loss: 0.689461\n",
      "loss: 0.689456\n",
      "loss: 0.689452\n",
      "loss: 0.689447\n",
      "loss: 0.689443\n",
      "loss: 0.689438\n",
      "loss: 0.689434\n",
      "loss: 0.689429\n",
      "loss: 0.689425\n",
      "loss: 0.689420\n",
      "loss: 0.689416\n",
      "loss: 0.689411\n",
      "loss: 0.689407\n",
      "loss: 0.689402\n",
      "loss: 0.689397\n",
      "loss: 0.689393\n",
      "loss: 0.689388\n",
      "loss: 0.689384\n",
      "loss: 0.689379\n",
      "loss: 0.689375\n",
      "loss: 0.689370\n",
      "loss: 0.689366\n",
      "loss: 0.689361\n",
      "loss: 0.689356\n",
      "loss: 0.689352\n",
      "loss: 0.689347\n",
      "loss: 0.689343\n",
      "loss: 0.689338\n",
      "loss: 0.689334\n",
      "loss: 0.689329\n",
      "loss: 0.689324\n",
      "loss: 0.689320\n",
      "loss: 0.689315\n",
      "loss: 0.689311\n",
      "loss: 0.689306\n",
      "loss: 0.689301\n",
      "loss: 0.689297\n",
      "loss: 0.689292\n",
      "loss: 0.689288\n",
      "loss: 0.689283\n",
      "loss: 0.689278\n",
      "loss: 0.689274\n",
      "loss: 0.689269\n",
      "loss: 0.689264\n",
      "loss: 0.689260\n",
      "loss: 0.689255\n",
      "loss: 0.689251\n",
      "loss: 0.689246\n",
      "loss: 0.689241\n",
      "loss: 0.689237\n",
      "loss: 0.689232\n",
      "loss: 0.689227\n",
      "loss: 0.689223\n",
      "loss: 0.689218\n",
      "loss: 0.689213\n",
      "loss: 0.689209\n",
      "loss: 0.689204\n",
      "loss: 0.689199\n",
      "loss: 0.689195\n",
      "loss: 0.689190\n",
      "loss: 0.689185\n",
      "loss: 0.689181\n",
      "loss: 0.689176\n",
      "loss: 0.689171\n",
      "loss: 0.689167\n",
      "loss: 0.689162\n",
      "loss: 0.689157\n",
      "loss: 0.689153\n",
      "loss: 0.689148\n",
      "loss: 0.689143\n",
      "loss: 0.689138\n",
      "loss: 0.689134\n",
      "loss: 0.689129\n",
      "loss: 0.689124\n",
      "loss: 0.689120\n",
      "loss: 0.689115\n",
      "loss: 0.689110\n",
      "loss: 0.689105\n",
      "loss: 0.689101\n",
      "loss: 0.689096\n",
      "loss: 0.689091\n",
      "loss: 0.689086\n",
      "loss: 0.689082\n",
      "loss: 0.689077\n",
      "loss: 0.689072\n",
      "loss: 0.689067\n",
      "loss: 0.689063\n",
      "loss: 0.689058\n",
      "loss: 0.689053\n",
      "loss: 0.689048\n",
      "loss: 0.689044\n",
      "loss: 0.689039\n",
      "loss: 0.689034\n",
      "loss: 0.689029\n",
      "loss: 0.689025\n",
      "loss: 0.689020\n",
      "loss: 0.689015\n",
      "loss: 0.689010\n",
      "loss: 0.689005\n",
      "loss: 0.689001\n",
      "loss: 0.688996\n",
      "loss: 0.688991\n",
      "loss: 0.688986\n",
      "loss: 0.688981\n",
      "loss: 0.688977\n",
      "loss: 0.688972\n",
      "loss: 0.688967\n",
      "loss: 0.688962\n",
      "loss: 0.688957\n",
      "loss: 0.688952\n",
      "loss: 0.688948\n",
      "loss: 0.688943\n",
      "loss: 0.688938\n",
      "loss: 0.688933\n",
      "loss: 0.688928\n",
      "loss: 0.688923\n",
      "loss: 0.688919\n",
      "loss: 0.688914\n",
      "loss: 0.688909\n",
      "loss: 0.688904\n",
      "loss: 0.688899\n",
      "loss: 0.688894\n",
      "loss: 0.688889\n",
      "loss: 0.688885\n",
      "loss: 0.688880\n",
      "loss: 0.688875\n",
      "loss: 0.688870\n",
      "loss: 0.688865\n",
      "loss: 0.688860\n",
      "loss: 0.688855\n",
      "loss: 0.688850\n",
      "loss: 0.688845\n",
      "loss: 0.688841\n",
      "loss: 0.688836\n",
      "loss: 0.688831\n",
      "loss: 0.688826\n",
      "loss: 0.688821\n",
      "loss: 0.688816\n",
      "loss: 0.688811\n",
      "loss: 0.688806\n",
      "loss: 0.688801\n",
      "loss: 0.688796\n",
      "loss: 0.688791\n",
      "loss: 0.688786\n",
      "loss: 0.688781\n",
      "loss: 0.688777\n",
      "loss: 0.688772\n",
      "loss: 0.688767\n",
      "loss: 0.688762\n",
      "loss: 0.688757\n",
      "loss: 0.688752\n",
      "loss: 0.688747\n",
      "loss: 0.688742\n",
      "loss: 0.688737\n",
      "loss: 0.688732\n",
      "loss: 0.688727\n",
      "loss: 0.688722\n",
      "loss: 0.688717\n",
      "loss: 0.688712\n",
      "loss: 0.688707\n",
      "loss: 0.688702\n",
      "loss: 0.688697\n",
      "loss: 0.688692\n",
      "loss: 0.688687\n",
      "loss: 0.688682\n",
      "loss: 0.688677\n",
      "loss: 0.688672\n",
      "loss: 0.688667\n",
      "loss: 0.688662\n",
      "loss: 0.688657\n",
      "loss: 0.688652\n",
      "loss: 0.688647\n",
      "loss: 0.688642\n",
      "loss: 0.688637\n",
      "loss: 0.688632\n",
      "loss: 0.688627\n",
      "loss: 0.688622\n",
      "loss: 0.688617\n",
      "loss: 0.688612\n",
      "loss: 0.688606\n",
      "loss: 0.688601\n",
      "loss: 0.688596\n",
      "loss: 0.688591\n",
      "loss: 0.688586\n",
      "loss: 0.688581\n",
      "loss: 0.688576\n",
      "loss: 0.688571\n",
      "loss: 0.688566\n",
      "loss: 0.688561\n",
      "loss: 0.688556\n",
      "loss: 0.688551\n",
      "loss: 0.688545\n",
      "loss: 0.688540\n",
      "loss: 0.688535\n",
      "loss: 0.688530\n",
      "loss: 0.688525\n",
      "loss: 0.688520\n",
      "loss: 0.688515\n",
      "loss: 0.688510\n",
      "loss: 0.688505\n",
      "loss: 0.688499\n",
      "loss: 0.688494\n",
      "loss: 0.688489\n",
      "loss: 0.688484\n",
      "loss: 0.688479\n",
      "loss: 0.688474\n",
      "loss: 0.688469\n",
      "loss: 0.688463\n",
      "loss: 0.688458\n",
      "loss: 0.688453\n",
      "loss: 0.688448\n",
      "loss: 0.688443\n",
      "loss: 0.688438\n",
      "loss: 0.688432\n",
      "loss: 0.688427\n",
      "loss: 0.688422\n",
      "loss: 0.688417\n",
      "loss: 0.688412\n",
      "loss: 0.688406\n",
      "loss: 0.688401\n",
      "loss: 0.688396\n",
      "loss: 0.688391\n",
      "loss: 0.688386\n",
      "loss: 0.688380\n",
      "loss: 0.688375\n",
      "loss: 0.688370\n",
      "loss: 0.688365\n",
      "loss: 0.688360\n",
      "loss: 0.688354\n",
      "loss: 0.688349\n",
      "loss: 0.688344\n",
      "loss: 0.688339\n",
      "loss: 0.688333\n",
      "loss: 0.688328\n",
      "loss: 0.688323\n",
      "loss: 0.688318\n",
      "loss: 0.688312\n",
      "loss: 0.688307\n",
      "loss: 0.688302\n",
      "loss: 0.688296\n",
      "loss: 0.688291\n",
      "loss: 0.688286\n",
      "loss: 0.688281\n",
      "loss: 0.688275\n",
      "loss: 0.688270\n",
      "loss: 0.688265\n",
      "loss: 0.688259\n",
      "loss: 0.688254\n",
      "loss: 0.688249\n",
      "loss: 0.688244\n",
      "loss: 0.688238\n",
      "loss: 0.688233\n",
      "loss: 0.688228\n",
      "loss: 0.688222\n",
      "loss: 0.688217\n",
      "loss: 0.688212\n",
      "loss: 0.688206\n",
      "loss: 0.688201\n",
      "loss: 0.688196\n",
      "loss: 0.688190\n",
      "loss: 0.688185\n",
      "loss: 0.688180\n",
      "loss: 0.688174\n",
      "loss: 0.688169\n",
      "loss: 0.688163\n",
      "loss: 0.688158\n",
      "loss: 0.688153\n",
      "loss: 0.688147\n",
      "loss: 0.688142\n",
      "loss: 0.688137\n",
      "loss: 0.688131\n",
      "loss: 0.688126\n",
      "loss: 0.688120\n",
      "loss: 0.688115\n",
      "loss: 0.688110\n",
      "loss: 0.688104\n",
      "loss: 0.688099\n",
      "loss: 0.688093\n",
      "loss: 0.688088\n",
      "loss: 0.688082\n",
      "loss: 0.688077\n",
      "loss: 0.688072\n",
      "loss: 0.688066\n",
      "loss: 0.688061\n",
      "loss: 0.688055\n",
      "loss: 0.688050\n",
      "loss: 0.688044\n",
      "loss: 0.688039\n",
      "loss: 0.688033\n",
      "loss: 0.688028\n",
      "loss: 0.688023\n",
      "loss: 0.688017\n",
      "loss: 0.688012\n",
      "loss: 0.688006\n",
      "loss: 0.688001\n",
      "loss: 0.687995\n",
      "loss: 0.687990\n",
      "loss: 0.687984\n",
      "loss: 0.687979\n",
      "loss: 0.687973\n",
      "loss: 0.687968\n",
      "loss: 0.687962\n",
      "loss: 0.687957\n",
      "loss: 0.687951\n",
      "loss: 0.687946\n",
      "loss: 0.687940\n",
      "loss: 0.687935\n",
      "loss: 0.687929\n",
      "loss: 0.687923\n",
      "loss: 0.687918\n",
      "loss: 0.687912\n",
      "loss: 0.687907\n",
      "loss: 0.687901\n",
      "loss: 0.687896\n",
      "loss: 0.687890\n",
      "loss: 0.687885\n",
      "loss: 0.687879\n",
      "loss: 0.687873\n",
      "loss: 0.687868\n",
      "loss: 0.687862\n",
      "loss: 0.687857\n",
      "loss: 0.687851\n",
      "loss: 0.687845\n",
      "loss: 0.687840\n",
      "loss: 0.687834\n",
      "loss: 0.687829\n",
      "loss: 0.687823\n",
      "loss: 0.687817\n",
      "loss: 0.687812\n",
      "loss: 0.687806\n",
      "loss: 0.687801\n",
      "loss: 0.687795\n",
      "loss: 0.687789\n",
      "loss: 0.687784\n",
      "loss: 0.687778\n",
      "loss: 0.687772\n",
      "loss: 0.687767\n",
      "loss: 0.687761\n",
      "loss: 0.687755\n",
      "loss: 0.687750\n",
      "loss: 0.687744\n",
      "loss: 0.687738\n",
      "loss: 0.687733\n",
      "loss: 0.687727\n",
      "loss: 0.687721\n",
      "loss: 0.687716\n",
      "loss: 0.687710\n",
      "loss: 0.687704\n",
      "loss: 0.687699\n",
      "loss: 0.687693\n",
      "loss: 0.687687\n",
      "loss: 0.687681\n",
      "loss: 0.687676\n",
      "loss: 0.687670\n",
      "loss: 0.687664\n",
      "loss: 0.687658\n",
      "loss: 0.687653\n",
      "loss: 0.687647\n",
      "loss: 0.687641\n",
      "loss: 0.687635\n",
      "loss: 0.687630\n",
      "loss: 0.687624\n",
      "loss: 0.687618\n",
      "loss: 0.687612\n",
      "loss: 0.687607\n",
      "loss: 0.687601\n",
      "loss: 0.687595\n",
      "loss: 0.687589\n",
      "loss: 0.687584\n",
      "loss: 0.687578\n",
      "loss: 0.687572\n",
      "loss: 0.687566\n",
      "loss: 0.687560\n",
      "loss: 0.687555\n",
      "loss: 0.687549\n",
      "loss: 0.687543\n",
      "loss: 0.687537\n",
      "loss: 0.687531\n",
      "loss: 0.687525\n",
      "loss: 0.687520\n",
      "loss: 0.687514\n",
      "loss: 0.687508\n",
      "loss: 0.687502\n",
      "loss: 0.687496\n",
      "loss: 0.687490\n",
      "loss: 0.687484\n",
      "loss: 0.687479\n",
      "loss: 0.687473\n",
      "loss: 0.687467\n",
      "loss: 0.687461\n",
      "loss: 0.687455\n",
      "loss: 0.687449\n",
      "loss: 0.687443\n",
      "loss: 0.687437\n",
      "loss: 0.687431\n",
      "loss: 0.687426\n",
      "loss: 0.687420\n",
      "loss: 0.687414\n",
      "loss: 0.687408\n",
      "loss: 0.687402\n",
      "loss: 0.687396\n",
      "loss: 0.687390\n",
      "loss: 0.687384\n",
      "loss: 0.687378\n",
      "loss: 0.687372\n",
      "loss: 0.687366\n",
      "loss: 0.687360\n",
      "loss: 0.687354\n",
      "loss: 0.687348\n",
      "loss: 0.687342\n",
      "loss: 0.687336\n",
      "loss: 0.687330\n",
      "loss: 0.687324\n",
      "loss: 0.687318\n",
      "loss: 0.687313\n",
      "loss: 0.687307\n",
      "loss: 0.687301\n",
      "loss: 0.687295\n",
      "loss: 0.687288\n",
      "loss: 0.687282\n",
      "loss: 0.687276\n",
      "loss: 0.687270\n",
      "loss: 0.687264\n",
      "loss: 0.687258\n",
      "loss: 0.687252\n",
      "loss: 0.687246\n",
      "loss: 0.687240\n",
      "loss: 0.687234\n",
      "loss: 0.687228\n",
      "loss: 0.687222\n",
      "loss: 0.687216\n",
      "loss: 0.687210\n",
      "loss: 0.687204\n",
      "loss: 0.687198\n",
      "loss: 0.687192\n",
      "loss: 0.687186\n",
      "loss: 0.687180\n",
      "loss: 0.687173\n",
      "loss: 0.687167\n",
      "loss: 0.687161\n",
      "loss: 0.687155\n",
      "loss: 0.687149\n",
      "loss: 0.687143\n",
      "loss: 0.687137\n",
      "loss: 0.687131\n",
      "loss: 0.687124\n",
      "loss: 0.687118\n",
      "loss: 0.687112\n",
      "loss: 0.687106\n",
      "loss: 0.687100\n",
      "loss: 0.687094\n",
      "loss: 0.687088\n",
      "loss: 0.687081\n",
      "loss: 0.687075\n",
      "loss: 0.687069\n",
      "loss: 0.687063\n",
      "loss: 0.687057\n",
      "loss: 0.687051\n",
      "loss: 0.687044\n",
      "loss: 0.687038\n",
      "loss: 0.687032\n",
      "loss: 0.687026\n",
      "loss: 0.687019\n",
      "loss: 0.687013\n",
      "loss: 0.687007\n",
      "loss: 0.687001\n",
      "loss: 0.686995\n",
      "loss: 0.686988\n",
      "loss: 0.686982\n",
      "loss: 0.686976\n",
      "loss: 0.686970\n",
      "loss: 0.686963\n",
      "loss: 0.686957\n",
      "loss: 0.686951\n",
      "loss: 0.686945\n",
      "loss: 0.686938\n",
      "loss: 0.686932\n",
      "loss: 0.686926\n",
      "loss: 0.686919\n",
      "loss: 0.686913\n",
      "loss: 0.686907\n",
      "loss: 0.686901\n",
      "loss: 0.686894\n",
      "loss: 0.686888\n",
      "loss: 0.686882\n",
      "loss: 0.686875\n",
      "loss: 0.686869\n",
      "loss: 0.686863\n",
      "loss: 0.686856\n",
      "loss: 0.686850\n",
      "loss: 0.686844\n",
      "loss: 0.686837\n",
      "loss: 0.686831\n",
      "loss: 0.686824\n",
      "loss: 0.686818\n",
      "loss: 0.686812\n",
      "loss: 0.686805\n",
      "loss: 0.686799\n",
      "loss: 0.686793\n",
      "loss: 0.686786\n",
      "loss: 0.686780\n",
      "loss: 0.686773\n",
      "loss: 0.686767\n",
      "loss: 0.686761\n",
      "loss: 0.686754\n",
      "loss: 0.686748\n",
      "loss: 0.686741\n",
      "loss: 0.686735\n",
      "loss: 0.686728\n",
      "loss: 0.686722\n",
      "loss: 0.686716\n",
      "loss: 0.686709\n",
      "loss: 0.686703\n",
      "loss: 0.686696\n",
      "loss: 0.686690\n",
      "loss: 0.686683\n",
      "loss: 0.686677\n",
      "loss: 0.686670\n",
      "loss: 0.686664\n",
      "loss: 0.686657\n",
      "loss: 0.686651\n",
      "loss: 0.686644\n",
      "loss: 0.686638\n",
      "loss: 0.686631\n",
      "loss: 0.686625\n",
      "loss: 0.686618\n",
      "loss: 0.686612\n",
      "loss: 0.686605\n",
      "loss: 0.686599\n",
      "loss: 0.686592\n",
      "loss: 0.686586\n",
      "loss: 0.686579\n",
      "loss: 0.686572\n",
      "loss: 0.686566\n",
      "loss: 0.686559\n",
      "loss: 0.686553\n",
      "loss: 0.686546\n",
      "loss: 0.686540\n",
      "loss: 0.686533\n",
      "loss: 0.686526\n",
      "loss: 0.686520\n",
      "loss: 0.686513\n",
      "loss: 0.686507\n",
      "loss: 0.686500\n",
      "loss: 0.686493\n",
      "loss: 0.686487\n",
      "loss: 0.686480\n",
      "loss: 0.686474\n",
      "loss: 0.686467\n",
      "loss: 0.686460\n",
      "loss: 0.686454\n",
      "loss: 0.686447\n",
      "loss: 0.686440\n",
      "loss: 0.686434\n",
      "loss: 0.686427\n",
      "loss: 0.686420\n",
      "loss: 0.686414\n",
      "loss: 0.686407\n",
      "loss: 0.686400\n",
      "loss: 0.686394\n",
      "loss: 0.686387\n",
      "loss: 0.686380\n",
      "loss: 0.686373\n",
      "loss: 0.686367\n",
      "loss: 0.686360\n",
      "loss: 0.686353\n",
      "loss: 0.686346\n",
      "loss: 0.686340\n",
      "loss: 0.686333\n",
      "loss: 0.686326\n",
      "loss: 0.686320\n",
      "loss: 0.686313\n",
      "loss: 0.686306\n",
      "loss: 0.686299\n",
      "loss: 0.686292\n",
      "loss: 0.686286\n",
      "loss: 0.686279\n",
      "loss: 0.686272\n",
      "loss: 0.686265\n",
      "loss: 0.686258\n",
      "loss: 0.686252\n",
      "loss: 0.686245\n",
      "loss: 0.686238\n",
      "loss: 0.686231\n",
      "loss: 0.686224\n",
      "loss: 0.686218\n",
      "loss: 0.686211\n",
      "loss: 0.686204\n",
      "loss: 0.686197\n",
      "loss: 0.686190\n",
      "loss: 0.686183\n",
      "loss: 0.686176\n",
      "loss: 0.686170\n",
      "loss: 0.686163\n",
      "loss: 0.686156\n",
      "loss: 0.686149\n",
      "loss: 0.686142\n",
      "loss: 0.686135\n",
      "loss: 0.686128\n",
      "loss: 0.686121\n",
      "loss: 0.686114\n",
      "loss: 0.686108\n",
      "loss: 0.686101\n",
      "loss: 0.686094\n",
      "loss: 0.686087\n",
      "loss: 0.686080\n",
      "loss: 0.686073\n",
      "loss: 0.686066\n",
      "loss: 0.686059\n",
      "loss: 0.686052\n",
      "loss: 0.686045\n",
      "loss: 0.686038\n",
      "loss: 0.686031\n",
      "loss: 0.686024\n",
      "loss: 0.686017\n",
      "loss: 0.686010\n",
      "loss: 0.686003\n",
      "loss: 0.685996\n",
      "loss: 0.685989\n",
      "loss: 0.685982\n",
      "loss: 0.685975\n",
      "loss: 0.685968\n",
      "loss: 0.685961\n",
      "loss: 0.685954\n",
      "loss: 0.685947\n",
      "loss: 0.685940\n",
      "loss: 0.685933\n",
      "loss: 0.685926\n",
      "loss: 0.685919\n",
      "loss: 0.685912\n",
      "loss: 0.685905\n",
      "loss: 0.685897\n",
      "loss: 0.685890\n",
      "loss: 0.685883\n",
      "loss: 0.685876\n",
      "loss: 0.685869\n",
      "loss: 0.685862\n",
      "loss: 0.685855\n",
      "loss: 0.685848\n",
      "loss: 0.685841\n",
      "loss: 0.685833\n",
      "loss: 0.685826\n",
      "loss: 0.685819\n",
      "loss: 0.685812\n",
      "loss: 0.685805\n",
      "loss: 0.685798\n",
      "loss: 0.685791\n",
      "loss: 0.685783\n",
      "loss: 0.685776\n",
      "loss: 0.685769\n",
      "loss: 0.685762\n",
      "loss: 0.685755\n",
      "loss: 0.685747\n",
      "loss: 0.685740\n",
      "loss: 0.685733\n",
      "loss: 0.685726\n",
      "loss: 0.685719\n",
      "loss: 0.685711\n",
      "loss: 0.685704\n",
      "loss: 0.685697\n",
      "loss: 0.685690\n",
      "loss: 0.685682\n",
      "loss: 0.685675\n",
      "loss: 0.685668\n",
      "loss: 0.685661\n",
      "loss: 0.685653\n",
      "loss: 0.685646\n",
      "loss: 0.685639\n",
      "loss: 0.685632\n",
      "loss: 0.685624\n",
      "loss: 0.685617\n",
      "loss: 0.685610\n",
      "loss: 0.685602\n",
      "loss: 0.685595\n",
      "loss: 0.685588\n",
      "loss: 0.685580\n",
      "loss: 0.685573\n",
      "loss: 0.685566\n",
      "loss: 0.685558\n",
      "loss: 0.685551\n",
      "loss: 0.685544\n",
      "loss: 0.685536\n",
      "loss: 0.685529\n",
      "loss: 0.685522\n",
      "loss: 0.685514\n",
      "loss: 0.685507\n",
      "loss: 0.685499\n",
      "loss: 0.685492\n",
      "loss: 0.685485\n",
      "loss: 0.685477\n",
      "loss: 0.685470\n",
      "loss: 0.685462\n",
      "loss: 0.685455\n",
      "loss: 0.685447\n",
      "loss: 0.685440\n",
      "loss: 0.685433\n",
      "loss: 0.685425\n",
      "loss: 0.685418\n",
      "loss: 0.685410\n",
      "loss: 0.685403\n",
      "loss: 0.685395\n",
      "loss: 0.685388\n",
      "loss: 0.685380\n",
      "loss: 0.685373\n",
      "loss: 0.685365\n",
      "loss: 0.685358\n",
      "loss: 0.685350\n",
      "loss: 0.685343\n",
      "loss: 0.685335\n",
      "loss: 0.685328\n",
      "loss: 0.685320\n",
      "loss: 0.685313\n",
      "loss: 0.685305\n",
      "loss: 0.685298\n",
      "loss: 0.685290\n",
      "loss: 0.685282\n",
      "loss: 0.685275\n",
      "loss: 0.685267\n",
      "loss: 0.685260\n",
      "loss: 0.685252\n",
      "loss: 0.685245\n",
      "loss: 0.685237\n",
      "loss: 0.685229\n",
      "loss: 0.685222\n",
      "loss: 0.685214\n",
      "loss: 0.685206\n",
      "loss: 0.685199\n",
      "loss: 0.685191\n",
      "loss: 0.685184\n",
      "loss: 0.685176\n",
      "loss: 0.685168\n",
      "loss: 0.685161\n",
      "loss: 0.685153\n",
      "loss: 0.685145\n",
      "loss: 0.685138\n",
      "loss: 0.685130\n",
      "loss: 0.685122\n",
      "loss: 0.685115\n",
      "loss: 0.685107\n",
      "loss: 0.685099\n",
      "loss: 0.685091\n",
      "loss: 0.685084\n",
      "loss: 0.685076\n",
      "loss: 0.685068\n",
      "loss: 0.685061\n",
      "loss: 0.685053\n",
      "loss: 0.685045\n",
      "loss: 0.685037\n",
      "loss: 0.685030\n",
      "loss: 0.685022\n",
      "loss: 0.685014\n",
      "loss: 0.685006\n",
      "loss: 0.684998\n",
      "loss: 0.684991\n",
      "loss: 0.684983\n",
      "loss: 0.684975\n",
      "loss: 0.684967\n",
      "loss: 0.684959\n",
      "loss: 0.684952\n",
      "loss: 0.684944\n",
      "loss: 0.684936\n",
      "loss: 0.684928\n",
      "loss: 0.684920\n",
      "loss: 0.684912\n",
      "loss: 0.684905\n",
      "loss: 0.684897\n",
      "loss: 0.684889\n",
      "loss: 0.684881\n",
      "loss: 0.684873\n",
      "loss: 0.684865\n",
      "loss: 0.684857\n",
      "loss: 0.684849\n",
      "loss: 0.684842\n",
      "loss: 0.684834\n",
      "loss: 0.684826\n",
      "loss: 0.684818\n",
      "loss: 0.684810\n",
      "loss: 0.684802\n",
      "loss: 0.684794\n",
      "loss: 0.684786\n",
      "loss: 0.684778\n",
      "loss: 0.684770\n",
      "loss: 0.684762\n",
      "loss: 0.684754\n",
      "loss: 0.684746\n",
      "loss: 0.684738\n",
      "loss: 0.684730\n",
      "loss: 0.684722\n",
      "loss: 0.684714\n",
      "loss: 0.684706\n",
      "loss: 0.684698\n",
      "loss: 0.684690\n",
      "loss: 0.684682\n",
      "loss: 0.684674\n",
      "loss: 0.684666\n",
      "loss: 0.684658\n",
      "loss: 0.684650\n",
      "loss: 0.684642\n",
      "loss: 0.684634\n",
      "loss: 0.684626\n",
      "loss: 0.684618\n",
      "loss: 0.684610\n",
      "loss: 0.684602\n",
      "loss: 0.684593\n",
      "loss: 0.684585\n",
      "loss: 0.684577\n",
      "loss: 0.684569\n",
      "loss: 0.684561\n",
      "loss: 0.684553\n",
      "loss: 0.684545\n",
      "loss: 0.684537\n",
      "loss: 0.684528\n",
      "loss: 0.684520\n",
      "loss: 0.684512\n",
      "loss: 0.684504\n",
      "loss: 0.684496\n",
      "loss: 0.684488\n",
      "loss: 0.684479\n",
      "loss: 0.684471\n",
      "loss: 0.684463\n",
      "loss: 0.684455\n",
      "loss: 0.684447\n",
      "loss: 0.684438\n",
      "loss: 0.684430\n",
      "loss: 0.684422\n",
      "loss: 0.684414\n",
      "loss: 0.684405\n",
      "loss: 0.684397\n",
      "loss: 0.684389\n",
      "loss: 0.684381\n",
      "loss: 0.684372\n",
      "loss: 0.684364\n",
      "loss: 0.684356\n",
      "loss: 0.684348\n",
      "loss: 0.684339\n",
      "loss: 0.684331\n",
      "loss: 0.684323\n",
      "loss: 0.684314\n",
      "loss: 0.684306\n",
      "loss: 0.684298\n",
      "loss: 0.684289\n",
      "loss: 0.684281\n",
      "loss: 0.684273\n",
      "loss: 0.684264\n",
      "loss: 0.684256\n",
      "loss: 0.684248\n",
      "loss: 0.684239\n",
      "loss: 0.684231\n",
      "loss: 0.684222\n",
      "loss: 0.684214\n",
      "loss: 0.684206\n",
      "loss: 0.684197\n",
      "loss: 0.684189\n",
      "loss: 0.684180\n",
      "loss: 0.684172\n",
      "loss: 0.684164\n",
      "loss: 0.684155\n",
      "loss: 0.684147\n",
      "loss: 0.684138\n",
      "loss: 0.684130\n",
      "loss: 0.684121\n",
      "loss: 0.684113\n",
      "loss: 0.684104\n",
      "loss: 0.684096\n",
      "loss: 0.684087\n",
      "loss: 0.684079\n",
      "loss: 0.684070\n",
      "loss: 0.684062\n",
      "loss: 0.684053\n",
      "loss: 0.684045\n",
      "loss: 0.684036\n",
      "loss: 0.684028\n",
      "loss: 0.684019\n",
      "loss: 0.684011\n",
      "loss: 0.684002\n",
      "loss: 0.683994\n",
      "loss: 0.683985\n",
      "loss: 0.683977\n",
      "loss: 0.683968\n",
      "loss: 0.683959\n",
      "loss: 0.683951\n",
      "loss: 0.683942\n",
      "loss: 0.683934\n",
      "loss: 0.683925\n",
      "loss: 0.683916\n",
      "loss: 0.683908\n",
      "loss: 0.683899\n",
      "loss: 0.683890\n",
      "loss: 0.683882\n",
      "loss: 0.683873\n",
      "loss: 0.683865\n",
      "loss: 0.683856\n",
      "loss: 0.683847\n",
      "loss: 0.683839\n",
      "loss: 0.683830\n",
      "loss: 0.683821\n",
      "loss: 0.683812\n",
      "loss: 0.683804\n",
      "loss: 0.683795\n",
      "loss: 0.683786\n",
      "loss: 0.683778\n",
      "loss: 0.683769\n",
      "loss: 0.683760\n",
      "loss: 0.683751\n",
      "loss: 0.683743\n",
      "loss: 0.683734\n",
      "loss: 0.683725\n",
      "loss: 0.683716\n",
      "loss: 0.683708\n",
      "loss: 0.683699\n",
      "loss: 0.683690\n",
      "loss: 0.683681\n",
      "loss: 0.683672\n",
      "loss: 0.683664\n",
      "loss: 0.683655\n",
      "loss: 0.683646\n",
      "loss: 0.683637\n",
      "loss: 0.683628\n",
      "loss: 0.683619\n",
      "loss: 0.683611\n",
      "loss: 0.683602\n",
      "loss: 0.683593\n",
      "loss: 0.683584\n",
      "loss: 0.683575\n",
      "loss: 0.683566\n",
      "loss: 0.683557\n",
      "loss: 0.683548\n",
      "loss: 0.683540\n",
      "loss: 0.683531\n",
      "loss: 0.683522\n",
      "loss: 0.683513\n",
      "loss: 0.683504\n",
      "loss: 0.683495\n",
      "loss: 0.683486\n",
      "loss: 0.683477\n",
      "loss: 0.683468\n",
      "loss: 0.683459\n",
      "loss: 0.683450\n",
      "loss: 0.683441\n",
      "loss: 0.683432\n",
      "loss: 0.683423\n",
      "loss: 0.683414\n",
      "loss: 0.683405\n",
      "loss: 0.683396\n",
      "loss: 0.683387\n",
      "loss: 0.683378\n",
      "loss: 0.683369\n",
      "loss: 0.683360\n",
      "loss: 0.683351\n",
      "loss: 0.683342\n",
      "loss: 0.683333\n",
      "loss: 0.683324\n",
      "loss: 0.683315\n",
      "loss: 0.683306\n",
      "loss: 0.683297\n",
      "loss: 0.683288\n",
      "loss: 0.683278\n",
      "loss: 0.683269\n",
      "loss: 0.683260\n",
      "loss: 0.683251\n",
      "loss: 0.683242\n",
      "loss: 0.683233\n",
      "loss: 0.683224\n",
      "loss: 0.683215\n",
      "loss: 0.683205\n",
      "loss: 0.683196\n",
      "loss: 0.683187\n",
      "loss: 0.683178\n",
      "loss: 0.683169\n",
      "loss: 0.683160\n",
      "loss: 0.683150\n",
      "loss: 0.683141\n",
      "loss: 0.683132\n",
      "loss: 0.683123\n",
      "loss: 0.683114\n",
      "loss: 0.683104\n",
      "loss: 0.683095\n",
      "loss: 0.683086\n",
      "loss: 0.683077\n",
      "loss: 0.683067\n",
      "loss: 0.683058\n",
      "loss: 0.683049\n",
      "loss: 0.683040\n",
      "loss: 0.683030\n",
      "loss: 0.683021\n",
      "loss: 0.683012\n",
      "loss: 0.683002\n",
      "loss: 0.682993\n",
      "loss: 0.682984\n",
      "loss: 0.682974\n",
      "loss: 0.682965\n",
      "loss: 0.682956\n",
      "loss: 0.682946\n",
      "loss: 0.682937\n",
      "loss: 0.682928\n",
      "loss: 0.682918\n",
      "loss: 0.682909\n",
      "loss: 0.682900\n",
      "loss: 0.682890\n",
      "loss: 0.682881\n",
      "loss: 0.682871\n",
      "loss: 0.682862\n",
      "loss: 0.682853\n",
      "loss: 0.682843\n",
      "loss: 0.682834\n",
      "loss: 0.682824\n",
      "loss: 0.682815\n",
      "loss: 0.682805\n",
      "loss: 0.682796\n",
      "loss: 0.682786\n"
     ]
    }
   ],
   "source": [
    "# Training our 2-layer model\n",
    "num_iters = 2000\n",
    "initialize_global_weights()\n",
    "for i in range(num_iters):\n",
    "    loss = train_or_evaluate(X_train, y_train, softmax, lr=1e-7, reg=1e-5)\n",
    "    if np.isinf(loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659176029963 0.677777777778\n"
     ]
    }
   ],
   "source": [
    "# Use trained weight to checkout train accuracy and val accuracy\n",
    "train_scores = train_or_evaluate(X_train)\n",
    "train_acc = (np.argmax(train_scores, axis=1) == y_train).mean()\n",
    "val_scores = train_or_evaluate(X_val)\n",
    "val_acc = (np.argmax(val_scores, axis=1) == y_val).mean() \n",
    "print(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass     Sex   Age  SibSp  Parch     Fare\n",
       "0          892       3    male  34.5      0      0   7.8292\n",
       "1          893       3  female  47.0      1      0   7.0000\n",
       "2          894       2    male  62.0      0      0   9.6875\n",
       "3          895       3    male  27.0      0      0   8.6625\n",
       "4          896       3  female  22.0      1      1  12.2875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    418\n",
       "Pclass         418\n",
       "Sex            418\n",
       "Age            418\n",
       "SibSp          418\n",
       "Parch          418\n",
       "Fare           418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map categorical variable to integer\n",
    "test.Sex.replace({\"male\":0, \"female\":1}, inplace=True)\n",
    "\n",
    "# Replace NA\n",
    "test.Age.fillna(modes[\"Age\"], inplace=True)\n",
    "test.Fare.fillna(modes[\"Fare\"], inplace=True)\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training set and validation set\n",
    "testdata = test.as_matrix()\n",
    "X_test = testdata[:, 1:].astype(np.float64)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Predictions\n",
    "test_yhat = np.argmax(train_or_evaluate(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"PassengerId\":test.PassengerId, \"Survived\":test_yhat}).to_csv(\"survival_submission-nn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
